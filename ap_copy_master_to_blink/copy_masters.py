"""
Main copy logic for ap-copy-master-to-blink

Generated By: Claude Code (Claude Sonnet 4.5)
"""

from pathlib import Path
from typing import Dict, List, Set, Tuple
import logging

from ap_common import get_metadata, copy_file, get_filenames
from ap_common.constants import (
    NORMALIZED_HEADER_CAMERA,
    NORMALIZED_HEADER_GAIN,
    NORMALIZED_HEADER_OFFSET,
    NORMALIZED_HEADER_SETTEMP,
    NORMALIZED_HEADER_READOUTMODE,
    NORMALIZED_HEADER_EXPOSURESECONDS,
    NORMALIZED_HEADER_FILTER,
    NORMALIZED_HEADER_DATE,
    TYPE_MASTER_DARK,
    TYPE_MASTER_BIAS,
    TYPE_MASTER_FLAT,
)

from .config import SUPPORTED_EXTENSIONS
from .matching import determine_required_masters

logger = logging.getLogger(__name__)


def get_date_directory(lights_dir: Path) -> Path:
    """
    Extract DATE directory from lights directory path.

    The lights are in: blink/TARGET/DATE_YYYY-MM-DD/FILTER_X/
    The DATE directory is: blink/TARGET/DATE_YYYY-MM-DD/

    Args:
        lights_dir: Path to directory containing light frames

    Returns:
        Path to DATE directory (parent of lights directory)
    """
    # Check if current directory is a FILTER directory
    if lights_dir.name.startswith("FILTER_"):
        # Parent should be DATE directory
        date_dir = lights_dir.parent
        if date_dir.name.startswith("DATE_"):
            return date_dir

    # If we're already in a DATE directory, return it
    if lights_dir.name.startswith("DATE_"):
        return lights_dir

    # Otherwise, search upward for DATE directory
    current = lights_dir
    while current.parent != current:
        if current.name.startswith("DATE_"):
            return current
        current = current.parent

    # Fallback: use the lights directory itself
    logger.warning(
        f"Could not find DATE directory for {lights_dir}, using lights directory"
    )
    return lights_dir


def scan_blink_directories(blink_dir: Path) -> List[Dict[str, str]]:
    """
    Scan blink directory tree for light frames and read their metadata.

    Args:
        blink_dir: Root blink directory to scan

    Returns:
        List of metadata dictionaries for all light frames found
    """
    logger.info(f"Scanning blink directory: {blink_dir}")

    # Get all light frame files
    light_files = get_filenames(
        blink_dir, extensions=SUPPORTED_EXTENSIONS, recursive=True
    )

    if not light_files:
        logger.warning(f"No light frames found in {blink_dir}")
        return []

    logger.info(f"Found {len(light_files)} light frames")

    # Read metadata for all light frames
    metadata_list = get_metadata(light_files)

    logger.info(f"Read metadata for {len(metadata_list)} light frames")

    return metadata_list


def group_lights_by_config(
    metadata_list: List[Dict[str, str]],
) -> Dict[Tuple, List[Dict[str, str]]]:
    """
    Group light frames by unique calibration requirements.

    Groups by: camera, gain, offset, settemp, readoutmode, exposureseconds, filter, date

    Args:
        metadata_list: List of metadata dictionaries for light frames

    Returns:
        Dictionary mapping calibration config tuple to list of metadata dicts
    """
    groups: Dict[Tuple, List[Dict[str, str]]] = {}

    for metadata in metadata_list:
        # Create key from calibration-relevant fields
        key = (
            metadata.get(NORMALIZED_HEADER_CAMERA),
            metadata.get(NORMALIZED_HEADER_GAIN),
            metadata.get(NORMALIZED_HEADER_OFFSET),
            metadata.get(NORMALIZED_HEADER_SETTEMP),
            metadata.get(NORMALIZED_HEADER_READOUTMODE),
            metadata.get(NORMALIZED_HEADER_EXPOSURESECONDS),
            metadata.get(NORMALIZED_HEADER_FILTER),
            metadata.get(NORMALIZED_HEADER_DATE),
        )

        if key not in groups:
            groups[key] = []

        groups[key].append(metadata)

    logger.info(
        f"Grouped {len(metadata_list)} lights into {len(groups)} unique configurations"
    )

    return groups


def copy_master_to_blink(
    master_metadata: Dict[str, str],
    dest_dir: Path,
    dry_run: bool = False,
) -> bool:
    """
    Copy master calibration frame to blink directory.

    Args:
        master_metadata: Metadata dict for master frame (includes 'filepath')
        dest_dir: Destination directory (DATE directory)
        dry_run: If True, log action but don't copy

    Returns:
        True if copied (or would be copied in dry-run), False if skipped
    """
    source_path = Path(master_metadata["filepath"])
    dest_path = dest_dir / source_path.name

    if dest_path.exists():
        logger.debug(f"Master already exists, skipping: {dest_path.name}")
        return False

    if dry_run:
        logger.info(f"[DRY-RUN] Would copy: {source_path.name} -> {dest_dir}")
        return True

    logger.info(f"Copying: {source_path.name} -> {dest_dir}")
    copy_file(source_path, dest_path)
    return True


def process_blink_directory(
    library_dir: Path,
    blink_dir: Path,
    dry_run: bool = False,
) -> Dict[str, int]:
    """
    Main orchestration logic to copy masters to blink directories.

    Args:
        library_dir: Path to calibration library
        blink_dir: Path to blink directory tree
        dry_run: If True, log actions but don't copy files

    Returns:
        Dictionary with summary statistics:
        - darks_copied: Number of darks copied
        - biases_copied: Number of biases copied
        - flats_copied: Number of flats copied
        - darks_missing: Number of unique configs missing darks
        - biases_missing: Number of unique configs missing biases (when needed)
        - flats_missing: Number of unique configs missing flats
        - configs_processed: Number of unique calibration configs processed
    """
    stats = {
        "darks_copied": 0,
        "biases_copied": 0,
        "flats_copied": 0,
        "darks_missing": 0,
        "biases_missing": 0,
        "flats_missing": 0,
        "configs_processed": 0,
    }

    # Scan for light frames
    metadata_list = scan_blink_directories(blink_dir)

    if not metadata_list:
        logger.warning("No light frames found to process")
        return stats

    # Group by calibration configuration
    groups = group_lights_by_config(metadata_list)
    stats["configs_processed"] = len(groups)

    # Track which DATE directories we've processed to avoid duplicate copies
    processed_masters: Dict[Path, Set[str]] = {}

    # Process each unique configuration
    for config_key, lights in groups.items():
        # Use first light's metadata as representative for this group
        light_metadata = lights[0]

        logger.info(
            f"\nProcessing configuration: "
            f"camera={light_metadata.get(NORMALIZED_HEADER_CAMERA)}, "
            f"gain={light_metadata.get(NORMALIZED_HEADER_GAIN)}, "
            f"exposure={light_metadata.get(NORMALIZED_HEADER_EXPOSURESECONDS)}s, "
            f"filter={light_metadata.get(NORMALIZED_HEADER_FILTER)}, "
            f"date={light_metadata.get(NORMALIZED_HEADER_DATE)}"
        )

        # Find required masters
        masters = determine_required_masters(library_dir, light_metadata)
        dark = masters[TYPE_MASTER_DARK]
        bias = masters[TYPE_MASTER_BIAS]
        flat = masters[TYPE_MASTER_FLAT]

        # Get destination directory (DATE directory)
        lights_dir = Path(light_metadata["filepath"]).parent
        date_dir = get_date_directory(lights_dir)

        # Ensure we have a tracking set for this DATE directory
        if date_dir not in processed_masters:
            processed_masters[date_dir] = set()

        # Copy dark (if found and not already copied)
        if dark:
            dark_name = Path(dark["filepath"]).name
            if dark_name not in processed_masters[date_dir]:
                if copy_master_to_blink(dark, date_dir, dry_run):
                    stats["darks_copied"] += 1
                    processed_masters[date_dir].add(dark_name)
        else:
            stats["darks_missing"] += 1
            logger.warning(
                f"Missing dark for exposure={light_metadata.get(NORMALIZED_HEADER_EXPOSURESECONDS)}s"
            )

        # Copy bias (if needed and found and not already copied)
        if bias:
            bias_name = Path(bias["filepath"]).name
            if bias_name not in processed_masters[date_dir]:
                if copy_master_to_blink(bias, date_dir, dry_run):
                    stats["biases_copied"] += 1
                    processed_masters[date_dir].add(bias_name)

        # Copy flat (if found and not already copied)
        if flat:
            flat_name = Path(flat["filepath"]).name
            if flat_name not in processed_masters[date_dir]:
                if copy_master_to_blink(flat, date_dir, dry_run):
                    stats["flats_copied"] += 1
                    processed_masters[date_dir].add(flat_name)
        else:
            stats["flats_missing"] += 1
            logger.warning(
                f"Missing flat for filter={light_metadata.get(NORMALIZED_HEADER_FILTER)}, "
                f"date={light_metadata.get(NORMALIZED_HEADER_DATE)}"
            )

    return stats
