"""
Main copy logic for ap-copy-master-to-blink

Generated By: Claude Code (Claude Sonnet 4.5)
"""

from pathlib import Path
from typing import Dict, List, Set, Tuple
import logging

from ap_common import get_metadata, get_filtered_metadata, copy_file
from ap_common.progress import progress_iter
from ap_common.constants import (
    NORMALIZED_HEADER_CAMERA,
    NORMALIZED_HEADER_GAIN,
    NORMALIZED_HEADER_OFFSET,
    NORMALIZED_HEADER_SETTEMP,
    NORMALIZED_HEADER_READOUTMODE,
    NORMALIZED_HEADER_EXPOSURESECONDS,
    NORMALIZED_HEADER_FILTER,
    NORMALIZED_HEADER_DATE,
    NORMALIZED_HEADER_FILENAME,
    NORMALIZED_HEADER_TYPE,
    TYPE_LIGHT,
    TYPE_MASTER_DARK,
    TYPE_MASTER_BIAS,
    TYPE_MASTER_FLAT,
)

from .config import SUPPORTED_EXTENSIONS
from .matching import determine_required_masters

logger = logging.getLogger(__name__)


def get_date_directory(lights_dir: Path) -> Path:
    """
    Extract DATE directory from lights directory path.

    The lights are in: blink/TARGET/DATE_YYYY-MM-DD/FILTER_X/
    The DATE directory is: blink/TARGET/DATE_YYYY-MM-DD/

    Args:
        lights_dir: Path to directory containing light frames

    Returns:
        Path to DATE directory (parent of lights directory)
    """
    # Check if current directory is a FILTER directory
    if lights_dir.name.startswith("FILTER_"):
        # Parent should be DATE directory
        date_dir = lights_dir.parent
        if date_dir.name.startswith("DATE_"):
            return date_dir

    # If we're already in a DATE directory, return it
    if lights_dir.name.startswith("DATE_"):
        return lights_dir

    # Otherwise, search upward for DATE directory
    current = lights_dir
    while current.parent != current:
        if current.name.startswith("DATE_"):
            return current
        current = current.parent

    # Fallback: use the lights directory itself
    logger.warning(
        f"Could not find DATE directory for {lights_dir}, using lights directory"
    )
    return lights_dir


def scan_blink_directories(
    blink_dir: Path, quiet: bool = False
) -> List[Dict[str, str]]:
    """
    Scan blink directory tree for light frames and read their metadata.

    Args:
        blink_dir: Root blink directory to scan
        quiet: Suppress progress output

    Returns:
        List of metadata dictionaries for all light frames found
    """
    logger.debug(f"Scanning blink directory: {blink_dir}")

    # Get metadata for light frames only (exclude master calibration frames)
    patterns = [rf".*\{ext}$" for ext in SUPPORTED_EXTENSIONS]
    metadata = get_filtered_metadata(
        dirs=[str(blink_dir)],
        filters={NORMALIZED_HEADER_TYPE: TYPE_LIGHT},
        profileFromPath=True,
        patterns=patterns,
        recursive=True,
        printStatus=not quiet,
    )

    if not metadata:
        logger.warning(f"No light frames found in {blink_dir}")
        return []

    # Convert metadata dict to list of dicts (get_filtered_metadata returns {filename: metadata})
    metadata_list = list(metadata.values())

    logger.debug(f"Found {len(metadata_list)} light frames")

    return metadata_list


def group_lights_by_config(
    metadata_list: List[Dict[str, str]],
) -> Dict[Tuple, List[Dict[str, str]]]:
    """
    Group light frames by unique calibration requirements.

    Groups by: camera, gain, offset, settemp, readoutmode, exposureseconds, filter, date

    Args:
        metadata_list: List of metadata dictionaries for light frames

    Returns:
        Dictionary mapping calibration config tuple to list of metadata dicts
    """
    groups: Dict[Tuple, List[Dict[str, str]]] = {}

    for metadata in metadata_list:
        # Create key from calibration-relevant fields
        key = (
            metadata.get(NORMALIZED_HEADER_CAMERA),
            metadata.get(NORMALIZED_HEADER_GAIN),
            metadata.get(NORMALIZED_HEADER_OFFSET),
            metadata.get(NORMALIZED_HEADER_SETTEMP),
            metadata.get(NORMALIZED_HEADER_READOUTMODE),
            metadata.get(NORMALIZED_HEADER_EXPOSURESECONDS),
            metadata.get(NORMALIZED_HEADER_FILTER),
            metadata.get(NORMALIZED_HEADER_DATE),
        )

        if key not in groups:
            groups[key] = []

        groups[key].append(metadata)

    logger.debug(
        f"Grouped {len(metadata_list)} lights into {len(groups)} unique configurations"
    )

    return groups


def copy_master_to_blink(
    master_metadata: Dict[str, str],
    dest_dir: Path,
    dry_run: bool = False,
) -> bool:
    """
    Copy master calibration frame to blink directory.

    Args:
        master_metadata: Metadata dict for master frame (includes 'filename')
        dest_dir: Destination directory (DATE directory)
        dry_run: If True, log action but don't copy

    Returns:
        True if copied (or would be copied in dry-run), False if skipped
    """
    source_path = Path(master_metadata[NORMALIZED_HEADER_FILENAME])
    dest_path = dest_dir / source_path.name

    if dest_path.exists():
        logger.debug(f"Master already exists, skipping: {dest_path.name}")
        return False

    if dry_run:
        logger.debug(f"[DRY-RUN] Would copy: {source_path.name} -> {dest_dir}")
        return True

    logger.debug(f"Copying: {source_path.name} -> {dest_dir}")
    copy_file(str(source_path), str(dest_path))
    return True


def process_blink_directory(
    library_dir: Path,
    blink_dir: Path,
    dry_run: bool = False,
    quiet: bool = False,
) -> Dict[str, int]:
    """
    Main orchestration logic to copy masters to blink directories.

    Args:
        library_dir: Path to calibration library
        blink_dir: Path to blink directory tree
        dry_run: If True, log actions but don't copy files
        quiet: Suppress progress output

    Returns:
        Dictionary with summary statistics:
        - frame_count: Total number of light frames
        - target_count: Number of unique targets
        - date_count: Number of unique dates
        - filter_count: Number of unique filters
        - darks_needed: Number of DATE directories that need darks
        - darks_present: Number of DATE directories that have darks
        - biases_needed: Number of DATE directories that need biases
        - biases_present: Number of DATE directories that have biases
        - flats_needed: Number of DATE directories that need flats
        - flats_present: Number of DATE directories that have flats
        - configs_processed: Number of unique calibration configs processed
    """
    stats = {
        "frame_count": 0,
        "target_count": 0,
        "date_count": 0,
        "filter_count": 0,
        "darks_needed": 0,
        "darks_present": 0,
        "biases_needed": 0,
        "biases_present": 0,
        "flats_needed": 0,
        "flats_present": 0,
        "configs_processed": 0,
    }

    # Scan for light frames
    metadata_list = scan_blink_directories(blink_dir, quiet=quiet)

    if not metadata_list:
        logger.warning("No light frames found to process")
        return stats

    # Extract organizational metrics
    targets = set()
    dates = set()
    filters = set()

    for metadata in metadata_list:
        # Extract target from path (blink_dir/TARGET/DATE/FILTER/file)
        light_path = Path(metadata[NORMALIZED_HEADER_FILENAME])
        try:
            # Get relative path from blink_dir
            rel_path = light_path.relative_to(blink_dir)
            # First component is TARGET
            target = rel_path.parts[0]
            targets.add(target)
        except (ValueError, IndexError):
            # Can't extract target from path
            pass

        # Extract date and filter from metadata
        if NORMALIZED_HEADER_DATE in metadata:
            dates.add(metadata[NORMALIZED_HEADER_DATE])
        if NORMALIZED_HEADER_FILTER in metadata:
            filters.add(metadata[NORMALIZED_HEADER_FILTER])

    stats["frame_count"] = len(metadata_list)
    stats["target_count"] = len(targets)
    stats["date_count"] = len(dates)
    stats["filter_count"] = len(filters)

    # Group by calibration configuration
    groups = group_lights_by_config(metadata_list)
    stats["configs_processed"] = len(groups)

    # Collect warnings to print after progress bar
    warnings = []

    # Track which DATE directories we've processed to avoid duplicate copies
    processed_masters: Dict[Path, Set[str]] = {}

    # Process each unique configuration
    for config_key, lights in progress_iter(
        groups.items(),
        desc="Processing configurations",
        unit="configs",
        enabled=not quiet,
        total=len(groups),
    ):
        # Use first light's metadata as representative for this group
        light_metadata = lights[0]

        logger.debug(
            f"Processing: "
            f"camera={light_metadata.get(NORMALIZED_HEADER_CAMERA)}, "
            f"gain={light_metadata.get(NORMALIZED_HEADER_GAIN)}, "
            f"exposure={light_metadata.get(NORMALIZED_HEADER_EXPOSURESECONDS)}s, "
            f"filter={light_metadata.get(NORMALIZED_HEADER_FILTER)}, "
            f"date={light_metadata.get(NORMALIZED_HEADER_DATE)}"
        )

        # Find required masters
        masters = determine_required_masters(library_dir, light_metadata)
        dark = masters[TYPE_MASTER_DARK]
        bias = masters[TYPE_MASTER_BIAS]
        flat = masters[TYPE_MASTER_FLAT]

        # Get all unique DATE directories in this group
        # (lights from multiple targets may share the same calibration config)
        date_dirs = set()
        for light in lights:
            lights_dir = Path(light[NORMALIZED_HEADER_FILENAME]).parent
            date_dir = get_date_directory(lights_dir)
            date_dirs.add(date_dir)

        # Copy masters to each DATE directory
        for date_dir in date_dirs:
            # Ensure we have a tracking set for this DATE directory
            if date_dir not in processed_masters:
                processed_masters[date_dir] = set()

            # Copy dark (if found and not already copied)
            stats["darks_needed"] += 1
            if dark:
                dark_name = Path(dark[NORMALIZED_HEADER_FILENAME]).name
                if dark_name not in processed_masters[date_dir]:
                    copy_master_to_blink(dark, date_dir, dry_run)
                    processed_masters[date_dir].add(dark_name)
                stats["darks_present"] += 1

            # Copy bias (if needed and found and not already copied)
            if bias:
                stats["biases_needed"] += 1
                bias_name = Path(bias[NORMALIZED_HEADER_FILENAME]).name
                if bias_name not in processed_masters[date_dir]:
                    copy_master_to_blink(bias, date_dir, dry_run)
                    processed_masters[date_dir].add(bias_name)
                stats["biases_present"] += 1

            # Copy flat (if found and not already copied)
            stats["flats_needed"] += 1
            if flat:
                flat_name = Path(flat[NORMALIZED_HEADER_FILENAME]).name
                if flat_name not in processed_masters[date_dir]:
                    copy_master_to_blink(flat, date_dir, dry_run)
                    processed_masters[date_dir].add(flat_name)
                stats["flats_present"] += 1

        # Collect missing master warnings (print after progress bar)
        if not dark:
            warnings.append(
                f"Missing dark for exposure={light_metadata.get(NORMALIZED_HEADER_EXPOSURESECONDS)}s "
                f"(run with --debug for search details)"
            )

        if not flat:
            warnings.append(
                f"Missing flat for filter={light_metadata.get(NORMALIZED_HEADER_FILTER)}, "
                f"date={light_metadata.get(NORMALIZED_HEADER_DATE)} "
                f"(run with --debug for search details)"
            )

    # Print collected warnings after progress bar completes
    for warning in warnings:
        logger.warning(warning)

    return stats
